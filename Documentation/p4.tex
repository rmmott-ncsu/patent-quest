\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[final]{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}

\usepackage{multicol}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\usepackage{graphicx}   % added for image importing
\usepackage{float}
\graphicspath{ {./images/} }





\title{Training on Edge: The Feasibility of Training Neural Networks on Raspberry Pis\newline}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.



\author{%
  Ralph Keyser \\
  Department of Computer Science \\
  North Carolina State University \\
  Raleigh, NC 27695 \\
  \texttt{rkeyser@ncsu.edu} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}
\maketitle
\begin{multicols}{2}



\begin{abstract}
Neural network training has become a big topic of discussion, especially in the last decade. Training deep neural networks is a quite intensive process; it can be time consuming, expensive, and even pollutive. The struggles with training deep neural networks has prompted a lot of innovation in this space and thought around how to optimize training and to discover new applications for training neural networks. Centralized training in data centers are often the chosen method for training new deep neural networks, but training on edge devices pose benefits not found in traditional neural network training scenarios. In this paper, we study the efficacy of training neural networks on ARM-based Raspberry Pi’s and discover that they prove to have real applicability in real-world training scenarios, providing great platforms for neural network education and for transfer learning on deep neural networks.
\end{abstract}

\section{Introduction}
\label{Introduction}

Deep neural networks are rather complex structures that take immense amounts of computational power to train. They may have millions, and often billions, of parameters, they are constructed with multiple hidden layers, and their training datasets are often large and require a lot of memory to enable quick storage and access for processing. The idea of training on embedded systems, such as the Raspberry Pi, may seem ridiculous as they are computationally not as capable as something like a GPU and do not have much RAM, but are worth investigating as devices with similar power and architectures are extremely prevalent in our world today. These devices can be found almost anywhere, from an airport terminal screen, to your thermostat, to your kitchen fridge. With all this idling computer power and with these devices showing up in more places every day, and especially in cases where machine learning inferencing is involved (such as security cameras, car dashboard cameras, etc), it’s worth considering if we can leverage these devices to further machine learning training and find ways to implement training effectively and efficiently.

For our experiment, we will be testing training scenarios across four different devices; the Raspberry Pi Zero 2w, the Raspberry Pi 3a+, the Raspberry Pi 4b, and the Raspberry Pi 5. The Pi Zero 2w, launched in October of 2021, is the smallest and most power efficient of the four devices. It boasts a 64-bit quad core ARM Cortex-A53 SoC that runs at 1GHz with 512MB of RAM\cite{website:rpi_zero}. The Pi 3a+, launched in March of 2018, is similar to the Pi Zero 2w in computational power boasting a 64-bit quad core ARM Cortex-A53 SoC, same as the Pi Zero 2w, but clocked at 1.4GHz along with 512MB of RAM\cite{website:rpi_3}. The Pi 4b, launched in June of 2019, and was a huge leap for the Raspberry Pi in computational performance. The new architecture of the Pi used a new 64-bit quad core ARM Cortex-A72 SoC that runs at 1.5GHz, offering a range of RAM options from 1GB to 8GB where our sample will have 4GB of RAM\cite{website:rpi_4}. The Pi 5, launched in September of 2023, is the most powerful Raspberry Pi from the test set with a 64-bit quad core ARM Cortex-A76 SoC that runs at 2.4GHz along with either 4GB or 8GB of RAM, where our sample will have 4GB of RAM\cite{website:rpi_5}.

These devices give us a broad range of computing capability that all falls within the embedded edge device class. With a wide array of processing power and with devices that have been optimized and improved in both performance and efficiency over the last 5 years, our goal is to gain insight into the trends of embedded system power and how effective deep neural network training is on embedded systems by analyzing performance on the Raspberry Pi and drawing conclusions based on the results while considering device age and architecture.


\section{Method}
\subsection{Background Knowledge on Edge Devices}
Since this study involved testing ‘edge’ devices, it’s important that we define an edge device as a device that sits at the boundary of data interaction across a network\cite{website:edge_device}.. These are often the devices that sit 1 or 0 hops away from the end user or endpoint. These devices range vastly in capability and power, so it is important that we focus our attention on ARM-based, low power devices for our study as these are very closely related to the selection of test devices that we are using. For machine learning purposes, edge devices can commonly be used for inference tasks, where models are trained on high powered data center servers, then which the model is used by edge devices ranging ARM-based doorbell cameras interpreting what objects are passing by the front of a home or whether it is a cell phone that is trying to better understand the meaning and intention behind the user’s voice commands. This test will be outside of the comfort zone of these devices and it will be a true test of their capabilities to see how the Raspberry Pis fare at training these neural networks.

\subsection{Selected Networks}
For this study, we have selected three different neural networks to test against the Raspberry Pis. For our first neural network selection, we are choosing LeNet (LeNet5). LeNet is one of the earliest examples (1998) of a neural network that, when trained, is capable of recognizing visual patterns such as calligraphy or writings of letters and numbers. LeNet consists of only 5 layers. In our test, we will be using the MNIST dataset to train LeNet. Our second neural network, MobileNetV2, is a much, much more intensive neural network and is aimed at being more efficient for mobile device inferencing. As MobileNetV2 is 53 layers deep and extremely memory intensive, all four Raspberry Pis were unable to start their training on the network, so for the purposes of testing transfer learning on the Raspberry Pis, we will supplement the network with the FashionMNIST dataset. Our third neural network, EfficientNetB0, is also very intensive, but is aimed at lightweight and balanced device inferencing. The network is 237 layers deep and also memory intensive, so similarly we will conduct transfer learning on the Raspberry Pis using the CIFAR10 dataset as our devices are unable to start their training on these devices.

\subsection{Transfer Learning}
As noted previously, our devices are unable to begin their training on MobileNetV2 and on EfficientNetB0, so we are investigating the efficacy and possibility of transfer learning on these devices. Transfer learning is done by taking a network with weights already pre trained against a large dataset, both of which have been pre-trained against ImageNet, and are honed by training a small dataset where the data is relevant to the specific use case the network might be applied to\cite{website:transfer_learning}. This helps narrow a network into a niche use case more accurately while saving time and resources by avoiding training for scenarios that the network will be unlikely to encounter.

\begin{figure}[H]
\includegraphics[width=6cm, height=6cm]{dog_cat}
\centering
\caption{Transfer learning on custom dataset}
\label{fig:dog_cat}
\end{figure}

This chart shows the training and validity accuracy of transfer learning using the Pi 5 on MobileNetV2 and a custom dataset of classified dogs and cats. Once the model was trained on the supplemented data, classifications for this use case were improved rather significantly. This is the great benefit of transfer learning, especially in the embedded space where certain embedded devices can be catered to their specific use cases and trained on new data.

\section{Experiments}
\subsection{LeNet on MNIST - Full Training}
For our first experiment, we will be training LeNet on the MNIST dataset without any pre-trained weights. LeNet is computationally inexpensive and very capable to be trained on all four devices. 

\begin{figure}[H]
\includegraphics[width=6cm, height=4cm]{pizero_lenet}
\centering
\caption{Pi Zero 2w training LeNet}
\label{fig:pizero_lenet}
\end{figure}
Over 10 epochs and roughly 120 seconds per epoch, the Pi Zero 2w was able to finish training in approximately 19 minutes and 9 seconds. 

\begin{figure}[H]
\includegraphics[width=6cm, height=4cm]{pi3_lenet}
\centering
\caption{Pi 3a+ training LeNet}
\label{fig:pi3_lenet}
\end{figure}
Over 10 epochs and roughly 110 seconds per epoch, the Pi 3a+ was able to finish training in approximately 18 minutes. 

\begin{figure}[H]
\includegraphics[width=6cm, height=4cm]{pi4_lenet}
\centering
\caption{Pi 4b training LeNet}
\label{fig:pi4_lenett}
\end{figure}
Over 10 epochs and roughly 30 seconds per epoch, the Pi 4b was able to finish training in approximately 4 minutes and 52 seconds. 

\begin{figure}[H]
\includegraphics[width=6cm, height=4cm]{pi5_lenet}
\centering
\caption{Pi 5 training LeNet}
\label{fig:pi5_lenet}
\end{figure}
Over 10 epochs and roughly 11 seconds per epoch, the Pi 5 was able to finish training in approximately 1 minute and 52 seconds. 

\begin{figure}[H]
\includegraphics[width=6cm, height=4cm]{res_lenet}
\centering
\caption{LeNet training results}
\label{fig:pres_lenet}
\end{figure}
For a small embedded device, the Pi Zero 2w impressively is able to fully train the network, alongside the pi 3a+. Even though both the Pi Zero 2w and 3a+ are the same architecture, the increased clock speed of the 3a+ is evident here and helps the 3a+ perform slightly better in this task. While only being a couple years newer than the Pi 3a+ in architecture, the Pi 4b is more than three times as efficient at training the network, and yet still the Pi 5 of 2023 produces the best result performing more than 10 times as well as the lowest contender, the Pi Zero 2w. 

\subsection{MobileNetV2 on FashionMNST}
For our second experiment, we will be conducting transfer learning with MobileNetV2 using the pre-trained weights from ImageNet and by training on the new dataset FashionMNST. Unfortunately, even though we are conducting transfer learning, the 512MB posed to be a hard limitation on the Pi Zero 2w and the Pi 3a+ as they both would finish half an epoch and then freeze as they hit their memory limits. This test will be conducted on the Pi 4b and Pi 5.

\begin{figure}[H]
\includegraphics[width=6cm, height=4cm]{pi4_mobile}
\centering
\caption{Pi 4b transfer training MobileNetV2}
\label{fig:pi4_mobile}
\end{figure}
Over 10 epochs and roughly 35 minutes per epoch, the Pi 4b was able to finish training in approximately 5 hours and 55 minutes.

\begin{figure}[H]
\includegraphics[width=6cm, height=4cm]{pi5_mobile}
\centering
\caption{Pi 5 transfer training MobileNetV2}
\label{fig:pi5_mobile}
\end{figure}
Over 10 epochs and roughly 19 minutes per epoch, the Pi 5 was able to finish training in approximately 2 hours and 51 minutes.

\subsection{EfficientNetB0 on CIFAR10}
For our third experiment, we will be conducting transfer learning with EfficientNetB0 using pre-trained weights from ImageNet and by training on the new dataset CIFAR10. This network was also unable to be executed on both the Pi Zero 2w and Pi 3a+, so testing will be conducted on the Pi 4b and Pi 5 as before.

\begin{figure}[H]
\includegraphics[width=6cm, height=4cm]{pi4_ef}
\centering
\caption{Pi 4b transfer training EfficientNetB0}
\label{fig:pi4_ef}
\end{figure}
Over 10 epochs and roughly 19 minutes per epoch, the Pi 4b was able to finish training in approximately 3 hours and 12 minutes.

\begin{figure}[H]
\includegraphics[width=6cm, height=4cm]{pi5_ef}
\centering
\caption{Pi 5 transfer training EfficientNetB0}
\label{fig:pi5_ef}
\end{figure}
Over 10 epochs and roughly 7 minutes and 48 seconds per epoch, the Pi 5 was able to finish training in approximately 1 hour and 18 minutes.

\section{Related Work}
Due to the sheer quantity of embedded systems that reside on the internet, embedded systems have a unique opportunity to take advantage of Federated Learning. Federated learning is a training method that distributes training data and workloads across multiple devices to improve training efficiency and data security\cite{website:fed_learning}. This is incredibly important as it pertains to data security as all data is able to stay local to the training device itself, so there is no risk of data leaking into the wrong hands, whether it be images within a private home or voice audio from device users. While this is the perfect use case for these devices, we were unfortunately unable to incorporate this into our study as unresolved dependency issues barred us from making any meaningful progress. Any future investigations would strongly benefit from exploring the application of federated learning on embedded devices such as the Raspberry Pis in this study. We assume that in a federated learning use case across these devices, devices similar to the Pi 4b and Pi 5 will be of most use and would benefit most from federated learning due solely to their higher memory amount. We figure that the Pi Zero 2w and Pi 3a+ would struggle with 512MB of RAM as they have been in the experiment use cases, and even though their processors would contribute to federated learning, the rest of the device platform is unlikely to be useful for this use case because of its memory.

\section{Conclusion}
From the results involving our three distinct experiments and our single explorative study, it’s clear that training deep neural networks on embedded devices, such as the Raspberry Pi, are completely feasible as long as a full unweighted deep neural network isn’t the subject at hand. The Raspberry Pi Zero 2w and Pi 3a+ were unable to run transfer learning training tasks on MobileNetV2 and EfficientNetB0 due to their memory constraints, but rather impressively were able to train the old and shallow LeNet. While there aren’t many real world applications to training on this level of low-powered computer processing, a device like the Pi Zero 2w at just \$15 poses a fantastic opportunity for educators to utilize these small computers in a learning environment. This could enable younger grade school children to learn about neural networks even earlier in their educational years and possibly even train their own neural network like LeNet for educational purposes. Even considering the inability for these devices to train full strength deep neural networks, we were still surprised at the ability of the Raspberry Pi 4b and 5 to be able to conduct rather intensive transfer learning tasks on decently large datasets. This has significant real world implications as embedded devices can often gather their own data in whichever application they are subjected to. As long as this data is classified and fed back to the devices to train for themselves, this application greatly benefits any device that deals with more unique data types and scenarios. Further, considering the technological leap that was the 5 years between the launch of the Pi 3a+ and the Pi 5 being nearly 10x as fast as the Pi 3a+, we’re hopeful that embedded systems will continue to advance in the coming years and continue to make improvements in efficiency and computational power, thus enabling more facets of machine learning and training neural networks. While not entirely in the manner that we expected, this study has confirmed our hypothesis that these devices are in fact capable of being utilized for training neural networks and can do so rather effectively.

\end{multicols}

\bibliographystyle{plain} 
\bibliography{p4} 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}